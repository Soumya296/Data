{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF3bZfc3tyv-"
   },
   "source": [
    "Implement neural network from scratch using python for the following datasets and predict the values for the following datasets:\n",
    "1. Boston House prices dataset: https://www.kaggle.com/vikrishnan/boston-house-prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df= pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)                                                              #load dataset\n",
    "m=df.shape[0]               # m--> number of rows\n",
    "n=df.shape[1]               # n--> number of columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "df.iloc[:,:]=sc.fit_transform(df.iloc[:,:])\n",
    "\n",
    "x_train, x_test, y_train, y_test = [i.to_numpy() for i in train_test_split(df.iloc[:,:n-1],df.iloc[:,-1],test_size=0.2)]\n",
    "# 80-20 ratio split of the data and convertion to numpy array\n",
    "\n",
    "\n",
    "print(\"Size of train_x : {}\".format(x_train.shape))\n",
    "print(\"Size of train_y : {}\".format(y_train.shape))\n",
    "print(\"Size of test_x : {}\".format(x_test.shape))\n",
    "print(\"Size of test_y : {}\".format(y_test.shape))         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Relu Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "print(\"Relu(5) : {}\".format(relu([5,-2,3,-1,4,0,-3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-50,100,5000),relu(np.linspace(-50,100,5000)),c = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing Parameters of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(layer_sizes):\n",
    "    params = {} #defining paramets as dictionary which contains weights and biases as W and B in the prefix\n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        params['W' + str(i)] = np.random.randn(layer_sizes[i], layer_sizes[i-1])*0.01\n",
    "        params['B' + str(i)] = np.random.randn(layer_sizes[i],1)*0.01\n",
    "    return params\n",
    "\n",
    "params_temp = initialize_params([13,0,20,10,1])\n",
    "params_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X_train, params):\n",
    "    layers = len(layer_sizes)-1 # Defining the number of layers\n",
    "    values = {} #defining as dictionary to store the values of Z and Activations required for the backpropagations\n",
    "    \n",
    "    for i in range(1, layers+1):\n",
    "        if i==1: #for first layer, the imput is multiplied with weights and aDDED with bias\n",
    "            values['Z' + str(i)] = np.dot(params['W' + str(i)], X_train) + params['B' + str(i)]\n",
    "            values['A' + str(i)] = relu(values['Z' + str(i)]) #computing activations\n",
    "        else:\n",
    "            values['Z' + str(i)] = np.dot(params['W' + str(i)], values['A' + str(i-1)]) + params['B' + str(i)]\n",
    "            if i==layers: #for last layer, activations are computed without applying relu\n",
    "                values['A' + str(i)] = values['Z' + str(i)]\n",
    "            else:\n",
    "                values['A' + str(i)] = relu(values['Z' + str(i)])\n",
    "    return values\n",
    "\n",
    "# val = forward_propagation(x_train.T,params_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(values, Y_train): #mean squared error computation\n",
    "    layers = len(layer_sizes)-1\n",
    "    Y_pred = values['A' + str(layers)]\n",
    "    cost = 1/(2*len(Y_train)) * (np.sum(np.square(Y_pred - Y_train)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Back Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(params, values, X_train, Y_train):\n",
    "    layers = len(layer_sizes)-1\n",
    "    m = len(Y_train)\n",
    "    grads = {}\n",
    "    for i in range(layers,0,-1):\n",
    "        if i==layers: #for last layer dz = da as no relu non-linearity has been applied\n",
    "            dA = 1/m * (values['A' + str(i)] - Y_train)\n",
    "            dZ = dA\n",
    "        else:\n",
    "            dA = np.dot(params['W' + str(i+1)].T, dZ) #for internal layers multiply the dz of next layer with the weights\n",
    "            dZ = np.multiply(dA, np.where(values['A' + str(i)]>=0, 1, 0)) # calculate dz using relu concept of backprop on da\n",
    "        if i==1:\n",
    "            grads['W' + str(i)] = 1/m * np.dot(dZ, X_train.T)\n",
    "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        else:\n",
    "            grads['W' + str(i)] = 1/m * np.dot(dZ,values['A' + str(i-1)].T)\n",
    "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Update Parameters with Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate):\n",
    "    layers = len(layer_sizes)-1\n",
    "    params_updated = {}\n",
    "    for i in range(1,layers+1):\n",
    "        params_updated['W' + str(i)] = params['W' + str(i)] - learning_rate * grads['W' + str(i)]\n",
    "        params_updated['B' + str(i)] = params['B' + str(i)] - learning_rate * grads['B' + str(i)]\n",
    "    return params_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_loss(x_test,y_test, params):\n",
    "    values_train = forward_propagation(x_test.T, params)\n",
    "    test_loss = np.sqrt(mean_squared_error(y_test, values_train['A' + str(len(layer_sizes)-1)].T))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, params):\n",
    "    values = forward_propagation(X.T, params)\n",
    "    predictions = values['A' + str(len(values)//2)].T\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Conjuring up a Neural Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSqY-bZG9Z8A",
    "outputId": "efdbbaaa-2549-4662-c7dc-e37b071dac29"
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, layer_sizes, num_iters, learning_rate):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    params = initialize_params(layer_sizes)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        values = forward_propagation(X_train.T, params)\n",
    "        \n",
    "        cost = compute_cost(values, Y_train.T)\n",
    "        test_cost = compute_test_loss(x_test,y_test,params)\n",
    "        \n",
    "        train_loss.append(cost)\n",
    "        test_loss.append(test_cost)\n",
    "        \n",
    "        grads = backward_propagation(params, values,X_train.T, Y_train.T)\n",
    "        params = update_params(params, grads, learning_rate)\n",
    "        \n",
    "        if(i%100 == 0):\n",
    "            print(\"Current Learning Rate is : {}\".format(learning_rate))\n",
    "            print('Cost at iteration ' + str(i+1) + ' = ' + str(cost) + '\\n')\n",
    "            if(i!=0):\n",
    "                learning_rate = learning_rate*0.95 #learning rate scheduler\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize = (15,8))\n",
    "    \n",
    "    ax[0].plot(range(num_iters),train_loss)\n",
    "    ax[0].set_title(\"training loss trend\")\n",
    "    ax[1].plot(range(num_iters),test_loss, color = 'red')\n",
    "    ax[1].set_title(\"test loss trend\")\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [13, 32, 64, 32, 8, 1]            #set layer sizes ; size of the first and last layer must\\\n",
    "                                                #be according to the features and expected output dimensions\n",
    "num_iters = 5000                                 #set number of training iterations over\n",
    "learning_rate = 0.1                              #set learning rate for gradient descent\n",
    "params = model(x_train, y_train, layer_sizes, num_iters, learning_rate)           #train the model on the traingin data\n",
    "test_rmse= compute_test_loss(x_test, y_test, params)  #get training and test accuracy\n",
    "\n",
    "print('Root Mean Squared Error on Testing Data = ' + str(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataScienceLabAssignment8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
